# k-Means and EM Algorithm

## K-means

聚类，一种非监督的学习方法。

假设数据 $\{x^{(1)}, ... x^{(N)}\}$ 在欧拉空间中，$\mathbf{x}^{(n)} \in \mathbb{R}^{D}$。每一个数据点都属于 K 个聚类中的一个，相同类中的点最相似，而不同类之间的点不相似。

K-means 的目标：找到聚类的中心 $\{m_k\}^K_{k=1}$，以及 assignment $\{r^{(n)}\}^N_{n=1}$，从而使得每一类所以的数据点 $\{x^{(n)}\}$ 到其属于的聚类中心的距离之和最小。

![W11 K-means Objective](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 K-means Objective.png)

注：assignment $r^{(n)}_k$ 是一个 one-hot (或 1-of-k) encoding 值。$r_{k}^{(n)}=\mathbb{I}\left[\mathbf{x}^{(n)}\right.$ is assigned to cluster $\left.k\right]$，即 $\mathbf{r}^{(n)}=[0, . ., 1, . ., 0]^{\top}$。

![W11 K-means Objective 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 K-means Objective 2.png)

上面括起来的式子中，虽然进行了 K 次，但实际上只有一个结果是非零的 (一个数据点只能属于一类)：

![W11 K-means Objective 3](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 K-means Objective 3.png)

### Alternating Minimization
现在，我们要对 k-means 进行优化：

![W11 optimize 1](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 optimize 1.png)

如果我们能确定聚类的中心 $\{m_k\}$，那么很容易能为每个点找到最好的 assignment $\{r^{(n)}\}$。

<img src="D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 optimize 2.png" alt="W11 optimize 2" style="zoom: 67%;" />

之后，如果我们确定了 assignment $\{r^{(n)}\}$，那么我们可以根据每个聚类的数据确定最好的聚类中心 $\{m_k\}$。我们可以通过所有属于该聚类的点的坐标，来确定最好的聚类中心。

![W11 optimize 3](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 optimize 3.png)

我们重复这样来确定聚类中心和 assignment，这就叫 alternating minimization。

### K-means Algorithm
![W11 kmeans](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 kmeans.png)

![W11 kmeans 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 kmeans 2.png)

注：assignment step 就是确定点是哪个类；refitting step 就是确定新的聚类中心在哪。

k-means 的每一次迭代，都会使类内点到中心的总距离 $J$ 变小。当中心不再变化时，k-means 便收敛了。

### Local Minima

由于 $J$ 是非凸函数，因此我们不能保证一定有最好的结果。k-means 可能被困在局部最小值中。

<img src="D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 local minima.png" alt="W11 local minima" style="zoom:50%;" />

## Soft K-means

相对于 hard assignment，我们可以使用 soft assignment，即让一个点可能属于多个聚类 (比如有 0.7 属于某类，有 0.3 属于另一类)。这样我们在 refitting step 时可以使用更多的点的信息。

![W11 Soft K-means Algorithm 1](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 Soft K-means Algorithm 1.png)

注：开始和 k-means 一样，随机选择 K 个聚类中心。然后对于每一个点，求它和每个聚类中心的距离，然后用 softmax 给出该点属于每个聚类的概率 (即权重)。现在的 $r^{(n)}$ 由 K 个权重组成。

![W11 Soft K-means Algorithm 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 Soft K-means Algorithm 2.png)

注：现在的 refitting 是每个点都参与，根据权重来计算点的贡献。

### The Generative Model

现在我们面论两个问题：

* 怎么设置 $\beta$
* 权重和宽度不相等的簇？

我们可以使用 generative model 来解决这些问题。

![W11 The Generative Model 1](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 The Generative Model 1.png)

注：上面干了两件事：确定从 K 个聚类中选择一个聚类 z 为 k 的概率为 $\pi_{k}$，以及确定在给定聚类 z 后生成 z 中的数据 x 的概率。

![W11 The Generative Model 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 The Generative Model 2.png)

注：上面可以得到 x 属于 k 类的概率。marginal 是说，假如给了头疼的概率 A 和感冒的概率 B，marginal P(A) 就是我不管我感冒不感冒，我其他什么因素都不考虑，我头疼的概率是多少。

![W11 The Generative Model 3](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 The Generative Model 3.png)

![W11 GMM](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 GMM.png)

接下来我们要算出数据 D 的最大似然：

![W11 The Generative Model 4](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 The Generative Model 4.png)

和 k-means 一样，如果我们知道每个点 $x^{(n)}$ 对应的聚类 $z^{(n)}$，那么我们很容易得到最大似然。

![W11 The Generative Model 5](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 The Generative Model 5.png)

接下来我们可以做和朴素贝叶斯中类似的操作：

![W11 The Generative Model 6](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 The Generative Model 6.png)

注：$\hat{\boldsymbol{\mu}}_{k}$ 得到的是聚类中所有点的均值；$\hat{\pi}_{k}$ 得到的是聚类中点的数量 (除以总数据量)。

接下来，我们可以计算出 x 属于哪个聚类：

![W11 The Generative Model 7](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 The Generative Model 7.png)

然后是另一种求 $\hat{\boldsymbol{\mu}}_{k}$ 和 $\hat{\pi}_{k}$ 的方法：

![W11 The Generative Model 8](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 The Generative Model 8.png)

### EM Algorithm for GMM

GMM 就是上面那些，现在我们提供一个整体的思路：

![W11 The Generative Model 9](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 The Generative Model 9.png)

我们先求出一个后验概率 $r^{(n)}_k$(或 $p(z=k|x)$)，即 k-means 中的 assignment step，得到每个点属于哪一类。之后我们得到参数 $\hat{\boldsymbol{\mu}}_{k}$ 和 $\hat{\pi}_{k}$ 来更新 $r^{(n)}_k$，即得到现在每个点属于哪一类。

因为由 E-step 和 M-step 组成，这个方法叫 EM algorithm。

![W11 EM Algorithm for GMM 1](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 EM Algorithm for GMM 1.png)

### Review

现在回顾一下我们之前干了什么：

![W11 Review 1](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 Review 1.png)

由于不知道 $z^{(n)}$，即 $x^{(n)}$ 属于哪一类：

![W11 Review 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 Review 2.png)

![W11 Review 3](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W11 Review 3.png)

