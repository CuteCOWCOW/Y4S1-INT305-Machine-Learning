# Support Vector Machine, SVM Loss and Softmax Loss

## Binary Classification with a Linear Model

![W4 Binary Classification with a Linear Model](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W4 Binary Classification with a Linear Model.png)

### Separating Hyperplanes

![W4 Separating Hyperplanes 1](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W4 Separating Hyperplanes 1.png)

Find the hyperplane:

![W4 Separating Hyperplanes 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W4 Separating Hyperplanes 2.png)

![W4 Separating Hyperplanes 3](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W4 Separating Hyperplanes 3.png)

### Optimal Separating Hyperplane

**Optimal Separating Hyperplane**: A hyperplane that separates two classes and maximizes the distance to the closest point from either class, i.e., maximize the **margin** of the classifier.

![W4 OSH](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W4 OSH.png)

### Geometry of Points and Planes

![W4 Geometry of Points and Planes](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W4 Geometry of Points and Planes.png)

注：为什么 [w 垂直于 decision hyperplane](https://stackoverflow.com/questions/10177330/why-is-weight-vector-orthogonal-to-decision-plane-in-neural-networks)。

The (signed) distance of a point $\mathbf{x}^{\prime}$ to the hyperplane is
$$
\frac{\mathbf{w}^{\top} \mathbf{x}^{\prime}+b}{\|\mathbf{w}\|_{2}}
$$

### Maximizing Margin as an Optimization Problem

The classification for the $i$-th data point is correct when
$$
\operatorname{sign}\left(\mathbf{w}^{\top} \mathbf{x}^{(i)}+b\right)=t^{(i)}
$$
This can be rewritten as
$$
t^{(i)}\left(\mathbf{w}^{\top} \mathbf{x}^{(i)}+b\right)>0
$$
注：$$ t^{(i)} \in\{-1,+1\} $$，$$ \mathbf{w}^{\top} \mathbf{x}^{(i)}+b $$ 和 $$ t^{(i)} $$​ 正负号一致，下同。

Enforcing a margin of $C$ :
$$
t^{(i)} \cdot \underbrace{\frac{\left(\mathbf{w}^{\top} \mathbf{x}^{(i)}+b\right)}{\|\mathbf{w}\|_{2}}}_{\text {signed distance }} \geq C
$$
![W4 Maximizing Margin as an Optimization Problem](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W4 Maximizing Margin as an Optimization Problem.png)

## SVM

![W4 SVM](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W4 SVM.png)

这就是 [SVM (Support Vector Machine)](https://blog.csdn.net/qq_31347869/article/details/88071930) 的原理：找到一个 hyperplane 使得类之间的距离最大。SVM-like algorithms are often called **max-margin** or **large-margin**.

找到这个 hyperplane 实际上只需参考距离最近的几个 training examples (or closest point)，而 closest point 到 hyperplane 的向量就是 support vector（closest point is the one with algebraic margin 1）。

### Non-Separable Data Points
![W4 Non-Separable data points](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W4 Non-Separable data points.png)

![W4 Maximizing Margin for non-separable data](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W4 Maximizing Margin for non-separable data.png)

![W4 Maximizing Margin for non-separable data 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W4 Maximizing Margin for non-separable data 2.png)

注：上式中分为两种情况：

1. point 被正确分类：

   这种情况下，$$ \xi_{i} $$ 为 0，式子还是 $$ \frac{t^{(i)}\left(\mathbf{w}^{\top} \mathbf{x}^{(i)}+b\right)}{\|\mathbf{w}\|_{2}} \geq C $$。

2. point 在 margin 内 或 被误分类：

   假如被误分类，在这种情况下，$$ \xi_{i} $$​​ > 1 (or $$ \xi_{i} > \frac{1}{\|w\|_{2}} $$​​ )。假如 $$ x^{(i)} $$​​ 的实际类别为 1，那么 $$ t^{(i)} $$​​ = 1；但被误分类后，$$ \left(\mathbf{w}^{\top} \mathbf{x}^{(i)}+b\right) $$​​ = -1，因此式子的左边现在变成了 $$ - \frac{(\mathbf{w}^{\top} \mathbf{x}^{\prime}+b)}{\|\mathbf{w}\|_{2}} $$​​。现在看式子右边，$$ 1 - \xi_{i} $$​​ 是点到决策面的距离（现在我们把 $$ \frac{1}{\|w\|_{2}} $$​​ 当作 1 来处理是为了方便，实际上 $$ \frac{1}{\|w\|_{2}} $$​​ 肯定不是 1，所以可以把它看作一个比例），$$ 1 - \xi_{i} $$​​ 为负号，然后 $$ C (1 - \xi_{i}) $$​​ 就是点到决策面的距离。

![W4 Maximizing Margin for non-separable data 3](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W4 Maximizing Margin for non-separable data 3.png)

注：我们一直的目标是 max margin，因为 margin = C = $$ \frac{1}{\|\mathbf{w}\|_{2}} $$，所以要 min $$ \frac{1}{\|\mathbf{w}\|_{2}} $$。我们当然不希望有一个被误分类的点离决策面很远，因此要 min $$ \sum_{i=1}^{N} \xi_{i} $$。

![W4 Maximizing Margin for non-separable data 4](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W4 Maximizing Margin for non-separable data 4.png)

注：1. 当 $$ \gamma = 0 $$，意为着不对 $$ \xi_{i} $$ 进行 min。那么假如有一个离决策面无限远的点，它到决策面的距离为 $$ \frac{\mathbf{w}^{\top} \mathbf{x}^{\prime}+b}{\|\mathbf{w}\|_{2}} = \infty $$，解为 w = 0。

2. 当 $$ \gamma = \infty $$，意味着将所有 $$ \xi_{i} $$ 都 min。假如我们已经将所有  $$ \xi_{i} $$ 都变成了 0，那么所有的点都不在 margin 内，也没有被误分类，这就是 hard-margin objective。

### From Margin Violation to Hinge Loss

![W4 From Margin Violation to Hinge Loss](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W4 From Margin Violation to Hinge Loss.png)

注：在 case 2 中，假如点被误分类到了 1 中，那么 $$ t^{(i)} = -1 $$​，它的作用是控制符号。$$ \left(\mathbf{w}^{\top} \mathbf{x}^{(i)}+b\right) $$​ 是一个正值，它代表着到决策面的距离，所以 $$ 1-t^{(i)}\left(\mathbf{w}^{\top} \mathbf{x}^{(i)}+b\right) $$​ 代表 $$ \gamma $$​。

![W4 From Margin Violation to Hinge Loss 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W4 From Margin Violation to Hinge Loss 2.png)

## Multiclass SVM loss

Given an example $\left(x_{i}, y_{i}\right)$​ where $\boldsymbol{x}_{i}$​, is the image and where $y_{i}$​ is the (integer) label, and using the shorthand for the scores vector: $s=f\left(x_{i}, W\right) = Wx_{i}$​​​.

The SVM loss has the form:
$$
L_{i}=\sum_{j \neq y_{i}} \max \left(0, s_{j}-s_{y_{i}}+1\right)
$$
注：$$ s_{j} $$ 是对 $$ x_{i} $$ 的预测值，$$ s_{y_{i}} $$ 是数据 $$ x_{i} $$ 的正确 label。

现在我们有 3 个 training example，和 3 个类别，它们的预测结果如下：

![W4 SVM loss 1](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W4 SVM loss 1.png)

然后计算出它们的 SVM loss：

![W4 SVM loss 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W4 SVM loss 2.png)

注：计算 SVM loss 时，不算正确的那一类（即 $$ j \neq y_{i} $$​）

![W4 SVM loss 3](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W4 SVM loss 3.png)

## Softmax Classifier (Multinomial Logistic Regression)

![W4 Softmax loss](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W4 Softmax loss.png)

注：$$ P\left(Y=k \mid X=x_{i}\right) $$ 意为在 $$ X=x_{i} $$ 的条件下，$$ Y=k $$​ 的概率。

![W4 Softmax 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W4 Softmax loss 2.png)

注：以 3.2 为例，从 unnormalized log probabilities 到 unnormalized probabilities，要进行 e^3.2^ = 24.5，即公式分子的操作。然后是 24.5 / (24.5 + 164.0 + 0.18) = 0.13，这是公式括号里面的操作。最后得到 Li 作为 loss。

## Softmax loss vs. SVM loss

![W4 Softmax vs SVM](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W4 Softmax vs SVM.png)

注：上面的后两个，SVM loss 都是 0，因此 SVM 无法很好的表现优化的情况。
