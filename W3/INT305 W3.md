# Linear Classifiers, Logistic Regression, Multiclass Classification
## Overview

![W3 Overview classification](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 Overview classification.png)

## Binary linear classification

![W3 Overview BLC](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 Overview BLC.png)

### Simplifications

我们对上面的 binary linear classification 进行简化。

* Eliminating the threshold（消除阈值）

  我们假设 threshold r = 0（该假设“不失一般性”，without loss of generality or WLOG，即该个例能代表普遍情况，而非一种特例）：
  $$
  \mathbf{w}^{\top} \mathbf{x}+b \geq r \quad \Longleftrightarrow \quad \mathbf{w}^{\top} \mathbf{x}+\underbrace{b-r}_{\triangleq w_{0}} \geq 0
  $$
  注：$$ \triangleq $$ 为恒等式，即 $$ b-r = w_{0} $$

* Eliminating the bias

  在上式中，b 是 bias。所以，可以和 linear regression 中一样：

  在 w 中增加一列 w~0~，使 w~0~ = b（实际上是 b - r），然后增加一个 dummy feature x~0~（x~0~ 永远为 1）。这样在运算的时候是 $$ w_{0}^{\top} x_{0}=b $$​。

* Simplified model

  ![W3 simplifications](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 simplifications.png)

### The Geometric Picture

![W3 GP 1](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 GP 1.png)

注：NOT example 是指 t 的结果为 NOT x~1~。

上图中蓝色的线为 decision boundary（在 2D 中，它是一条线；在高维中，它是一个 hyperplane）：$$ \left\{\mathbf{x}: \mathbf{w}^{\top} \mathbf{x}=0\right\} $$

decision boundary 划分出了两个 half-spaces：$$ H_{+}=\left\{\mathbf{x}: \mathbf{w}^{\top} \mathbf{x} \geq 0\right\}, \ H_{-}=\left\{\mathbf{x}: \mathbf{w}^{\top} \mathbf{x}<0\right\} $$​​​​​​​。图中红绿色的加减号所在的位置，对应右边表格中 x~0~ 和 x~1~ 的值。上方的红色区域是 negative space，下方的绿色区域是 positive space（对于 t）。

如果这个 boundary 可以完美地将 training examples 区分开，我们就说 data is linearly separable。

#### Weight Space

<img src="D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 GP 2.png" alt="W3 GP 2" style="zoom: 50%;" />

根据上表可得：

![W3 GP 3](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 GP 3.png)

因此，蓝色叉号所在的区域就是 w~0~ 和 w~1~ 可以取值的区域，叫做 feasible region。

![W3 GP AND](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 GP AND.png)

## Towards Logistic Regression

### Loss Function

![W3 TLR Loss Function 1](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 TLR Loss Function 1.png)

Usually, the cost $\mathcal{J}$ is the averaged loss over training examples; for 0 - 1 loss, this is the misclassification rate (错分类率):
$$
\mathcal{J}=\frac{1}{N} \sum_{i=1}^{N} \mathbb{I}\left[y^{(i)} \neq t^{(i)}\right]
$$

接下来，要对模型进行优化。

### Attempt 1：0-1 loss

![W3 0-1 loss](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 0-1 loss.png)

关于 0-1 loss，还有一个问题是：根据最终预测来定义，这本质上是不连续的。所以 0-1  loss 不能用于优化模型。

### Attempt 2：Linear Regression

有时，我们可以用更容易优化的 loss function 来替换当前的 loss function。这被称为 relaxation with a smooth surrogate loss function（用光滑的替代函数来放松）。
$$
\begin{aligned}
z &=\mathbf{w}^{\top} \mathbf{x} \\
\mathcal{L}_{\mathrm{SE}}(z, t) &=\frac{1}{2}(z-t)^{2}
\end{aligned}
$$
![W3 attempt2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 attempt2.png)

对于该 loss function，z 的阈值 为 $$ \frac{1}{2} $$ 时模型达到最优（看上图，如果阈值为其他数，拟合的线可能是斜的，这样会增大 loss，只有拟合线为 t = $$ \frac{1}{2} $$ 时，loss 最小）。

由于上述原因，使用该 loss function 不能很好的进行优化。

### Attempt 3：Logistic Activation Function

现在我们将 y 压缩到区间 [0, 1] 之间。

![W3 logit](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 logit.png)

注：logit 和 sigmoid [互为反函数](http://sofasofa.io/forum_main_post.php?postid=1004435)。

具有逻辑非线性 (logistic nonlinearity) 的线性模型称为：log-linear：
$$
\begin{aligned}
z &=\mathbf{w}^{\top} \mathbf{x} \\
y &=\sigma(z) \\
\mathcal{L}_{\mathrm{SE}}(y, t) &=\frac{1}{2}(y-t)^{2} .
\end{aligned}
$$
Used in this way, $$ \sigma $$ is called an **activation function**.

![W3 Logistic Activation Function problem](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 Logistic Activation Function problem.png)

* 由前面 $$ \sigma(z) $$ 的函数图像可知：For $$ z \ll 0 $$, $$ \sigma(z) \approx 0 $$.
* 所以 $$ \frac{\partial \mathcal{L}}{\partial z} \approx 0 \Longrightarrow \frac{\partial \mathcal{L}}{\partial w_{j}} \approx 0 \Longrightarrow $$ derivative w.r.t. (with respect to) $$ w_{j} $$ is small $$ \Longrightarrow w_{j} $$ is like a critical point
* 因此，用该 function，可能使模型向负轴方向优化，从而不能到达真正的 critical point

## Logistic Regression

### Cross-entropy loss

![W3 Cross-entropy loss](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 Cross-entropy loss.png)

注：交叉熵 loss 的机制是这样的：如果对于一个 sample，预测的是 1（一般还会包含一个置信度，比如有 90% 的把握认为是 1），但实际结果是 0，这时 cross-entropy loss 会对这个结果进行惩罚（一般，错误预测的置信度越大，惩罚越大），即让 loss 变得很大（见上图蓝线）。

### Logistic Regression

![W3 LR](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 LR.png)

### Gradient Descent for Logistic Regression
由于 logistic loss 是一个凸函数（**convex function**，如上图，曲线上方空间是凸出来的函数），所以我们可以用 gradient descent 来对其进行优化。

#### Gradient of Logistic Loss

我们首先将模型的权重进行合理的初始化，这里权重为 w，我们将 w 初始化为 0。

![W3 gradient of Logistic loss](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 gradient of Logistic loss.png)

## Multiclass Classification and Softmax Regression

### One-hot Encoding

![W3 one hot encoding](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 one hot encoding.png)

### Multiclass Linear Classification

分类任务经常不止一类，比如手写数字分类有 10 类。

![W3 MLC 1](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 MLC 1.png)

注：X 为 D 维矩阵，W 为 K $$ \times $$​ D 维矩阵，b 为 K 维矩阵，这样 WX + b 的结果为 K 维矩阵。

![W3 MLC 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 MLC 2.png)

![W3 MLC 3](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 MLC 3.png)

注：上式和 logistic classification 不一样。logistic classification 是之间算出 Wx 然后进行分类。而这里是先对每一类进行预测，z~k~ 的大小可以解释为模型倾向于将 k 预测结果的程度。

举个例子，假如做手写数字，一共有 10 类，输入 1 个有 D 个 feature 的 input。那么 w 就是 10 $$ \times $$ D，对于每一类都有特殊的 D 个权重。然后模型要对 input 进行 10 次预测，取其中最大的作为结果（在 one-hot encoding 中将相应的位置设为 1，即上面第二个式子所做的）。

![W3 MLC 4](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 MLC 4.png)

### Softmax Regression

![W3 Softmax Regression 1](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 Softmax Regression 1.png)

注：上式中，z~k~ 代表 z 中第 k 个元素，$$ \sum_{k^{\prime}} e^{z_{k^{\prime}}} $$​ 代表对 z 中每一个元素进行 $$ e^{z_{k^{\prime}}} $$​ 后的总和。

![W3 softmax regression 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 softmax regression 2.png)

上式叫 softmax-cross-entropy function。

### Gradient Descent

![W3 MLC GD](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 MLC GD.png)

## Limits of Linear Classification

![W3 Limits 1](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 Limits 1.png)

### Showing that XOR is not linearly separable

通过 contradiction（反证法）来证明：

![W3 Limits 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 Limits 2.png)

### Overcome

![W3 Limits 3](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W3 Limits 3.png)

