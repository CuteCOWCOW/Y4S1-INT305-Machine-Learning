# Linear Methods for Regression, Optimization
## Overview

Linear Regression:

* Task: predict scalar-valued targets (e.g. stock prices)
* Architecture: linear function of the inputs

**Modular approach**:

1. ***choose a model*** describing the relationships between variables of interest（选择描述我们感兴趣的元素和变量之间关系的模型）
2. ***define a loss function*** quantifying how bad the fit to the data is
3. ***choose a regularizer*** (正则化) saying how much we prefer different candidate models (or explanations of data)
4. fit a model that minimizes the loss function and satisfies the constraint/penalty imposed by the regularizer, possibly using an ***optimization algorithm***

## Linear Regression

### Model

![W2 LR model](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 LR model.png)

### Linear Regression

![W2 LR 1](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 LR 1.png)

注：$$t^{(i)}$$​ 是对于 $$x^{(i)}$$​ 的预测结果

![W2 LR 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 LR 2.png)

### Loss Function

![W2 Loss Function 1](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 Loss Function 1.png)

注：$$ y - t $$​ 是 residual（差距），我们希望它越小越好；

乘以 $$ \frac{1}{2} $$ 可以减轻运算量。

* **Cost function**：所有 training examples 的平均 loss

  ![W2 cost function](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 cost function.png)

  * 术语不同：有些叫 cost 或 average loss

  * 符号方面：将 $$ \frac{1}{2 N} \sum_{i=1}^{N}\left(y^{(i)}-t^{(i)}\right)^{2} $$​ 中的 $$ y^{(i)} $$​ 展开会得到
    $$
    \frac{1}{2 N} \sum_{i=1}^{N}\left(\sum_{j=1}^{D}\left(w_{j} x_{j}^{(i)}+b\right)-t^{(i)}\right)^{2}
    $$
    注：N 指有 N 个 training examples，D 指 x 有 D 个 features

### Vectorization

![W2 vectorization](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 vectorization.png)



![W2 vectorization 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 vectorization 2.png)

![W2 vectorization 3](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 vectorization 3.png)

### Minimization loss

Two commonly applied mathematical approaches:

* Algebraic (代数方法), e.g. using inequalities (不等式):

  ![W2 algebraic](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 algebraic.png)

* Calculus: minimum of a smooth function (if it is exists) occurs at a critical point (临界点), i.e. point where the derivative is zero (导数为零的点).

Solutions may be direct or iterative.

### Direct Solution

#### Linear Algebra

我们要寻找一个 w 来 minimize $$ \|\mathbf{X} \mathbf{w}-\mathbf{t}\|^{2} $$, or equivalently $$ \|\mathbf{X} \mathbf{w}-\mathbf{t}\|$$.
$$ \operatorname{range}(\mathbf{X})=\left\{\mathbf{X} \mathbf{w} \mid \mathbf{w} \in \mathbb{R}^{D}\right\} $$ is a D-dimensional subspace of $$ \mathbb{R}^{N} $$.

The closest point $\mathbf{y}^{*}=\mathbf{X} \mathbf{w}^{*}$ in subspace range$(\mathbf{X})$ of $\mathbb{R}^{N}$ to arbitrary point $\mathbf{t} \in \mathbb{R}^{N}$​ is found by orthogonal projection (直角投影).

![W2 linear algebra](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 linear algebra.png)

注：range(**X**) 和它所在的线，代表的是可能的 y 值。

![W2 linear algebra 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 linear algebra 2.png)

#### Calculus

![W2 partial derivative](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 partial derivative.png)

![W2 cost function derivatives](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 cost function derivatives.png)

注：这里是对 w，和 b 的偏导。

![W2 min cost function](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 min cost function.png)

![W2 gradient](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 gradient.png)

注：上式是 w 的梯度。

**梯度的方向，和函数最大增长率的方向一致。**

Analogue (类似情况) of second derivative (the "Hessian" matrix): $\nabla^{2} f(\mathbf{w}) \in \mathbb{R}^{D \times D}$ is a matrix with $\left[\nabla^{2} f(\mathbf{w})\right]_{i j}=\frac{\partial^{2}}{\partial w_{i} \partial w_{j}} f(\mathbf{w})$

注：海森矩阵，描述了函数的局部曲率，可判定多元函数的极值问题。用二次导数，可以确定梯度的变化趋势。

![W2 calculus](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 calculus.png)

注：矩阵求导不能用链式法则，有特殊的规则，因此：
$$
\begin{aligned}
\nabla_{w} \mathcal{J}(w) &=\frac{1}{2}\left(X^{2} w^{2}+t^{2}-2 X w t\right)^{\prime} \\
&=\frac{1}{2}\left(2 X^{2} w-2 X t\right) \\
&=X^{2} w-X t \\
&=X^{\top} Xw-X^{\top} t
\end{aligned}
$$

### Feature Mapping (Basis Expansion)

不是所有输入和输出的关系都是线性的，更多的可能是多项式关系 (polynomial relation)。如果样本量多，回归问题很复杂，而原始特征只有x1,x2。可以用多项式创建更多的特征x1、x2、x1^2^、x2^2^、... 。因为更多的特征进行回归时，得到的分割线可以是任意高阶函数的形状。

**通过一定的映射，把数据映射入高维**之后，便于分界。因为保留了映射前的特征，所以叫特征映射。

![W2 polynomial feature mapping](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 polynomial feature mapping.png)

注：在这个问题中，input 只有一个 feature，就是 x 轴的值。将这个feature 映射到 M 个维度中，就得到了上式。

![W2 underfitting and overfitting](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 underfitting and overfitting.png)

### Regularization

多项式的 degree M 控制了模型的复杂度。M 是一个超参数，和 KNN 中的 k 一样，因此我们同样可以使用 validation set 来进行调整。

不过，我们还有另外的方法：keep the model large, but ***regularize*** it

* **Regularizer**: a function that quantifies how much we prefer one hypothesis vs. another

#### L2 Regularization

我们可以选择使用 **L^2^ penalty** 作为 regularizer 来使 weights 变小。
$$
\mathcal{R}(\mathbf{w})=\frac{1}{2}\|\mathbf{w}\|_{2}^{2}=\frac{1}{2} \sum_{j} w_{j}^{2}
$$
正则化的代价函数（regularized cost function）在 数据的拟合 和 权重的范数（norm of the weights） 之间进行权衡。
$$
\mathcal{J}_{\operatorname{reg}}(\mathbf{w})=\mathcal{J}(\mathbf{w})+\lambda \mathcal{R}(\mathbf{w})=\mathcal{J}(\mathbf{w})+\frac{\lambda}{2} \sum_{j} w_{j}^{2}
$$

* If you fit training data poorly, $\mathcal{J}$ is large (误差大). If your optimal weights have high values, $\mathcal{R}$​ is large (模型复杂).
* Large $$\lambda$$​ penalizes  weight values more.
* 和 M 一样， $$\lambda$$ 也是 hyperparameter，可以通过 validation set 调整

#### L2 Regularized Least Squares: Ridge regression

![W2 L2 Regularized Least Squares Ridge regression](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 L2 Regularized Least Squares Ridge regression.png)

注：$$ \mathbf{I} $$ 表示单位矩阵，即在主对角线上元素均为 1，而其他元素都是 0

### Gradient Descent

很多时候，我们没有 direct solution。

![W2 gradient descent](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 gradient descent.png)

![W2 GD observe](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 GD observe.png)

![W2 GD](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 GD.png)

#### Gradient Descent under the L2 Regularization

![W2 Gradient Descent under the L2 Regularization](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 Gradient Descent under the L2 Regularization.png)

### Stochastic Gradient Descent

![W2 Stochastic Gradient Descent 1](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 Stochastic Gradient Descent 1.png)

计算梯度需要计算所有的 training example，这称为 **batch training**。

batch training 有时是不切实际的，如果你有一个大的数据集 $$ N \gg 1 $$（例如数百万个 training examples）。

**随机梯度下降 （SGD）**： 根据单个 training example 的梯度更新参数：

* 均匀随机地选择 $$ {i} $$（ $$ {i} $$ 是数据集中的一个 example）

* $$ \boldsymbol{\theta} \leftarrow \boldsymbol{\theta}-\alpha \frac{\partial \mathcal{L}^{(i)}}{\partial \boldsymbol{\theta}} $$

每个 SGD 更新的成本都与 N 无关，SGD 甚至在看到所有数据之前就可以取得重大进展。

数学理由：如果随机均匀地采样出一个 training example，随机梯度是对批次梯度（batch gradient）的公正估计（unbiased estimate）：
$$
\mathbb{E}\left[\frac{\partial \mathcal{L}^{(i)}}{\partial \boldsymbol{\theta}}\right]=\frac{1}{N} \sum_{i=1}^{N} \frac{\partial \mathcal{L}^{(i)}}{\partial \boldsymbol{\theta}}=\frac{\partial \mathcal{J}}{\partial \boldsymbol{\theta}}
$$
注：上式是对 batch gradient 的估计，结果和前面 By linearity 一致，所以是 unbiased estimate。

使用单一的 training example 来估计梯度可能方差会很高，我们可以随机选择中等大小的数据集 $$ \mathcal{M} \subset\{1, \ldots, N\} $$（称为 mini-batch），来进行训练，而这会使方差变小。

![W2 SGD 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W2 SGD 2.png)

#### SGD Learning Rate

在随机训练（stochastic training）中，学习率也会由于梯度的随机性而影响波动（fluctuations）。

训练策略：

* 在训练的早期使用高学习率，以便快速接最优解
* 逐渐降低学习率，减少 fluctuations

