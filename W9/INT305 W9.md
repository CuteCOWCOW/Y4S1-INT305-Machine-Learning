# Bagging & Boosting

## Bias-Variance Decomposition
我们从一个数据集中生成很多小数据集用来训练和测试。它们得到的 loss 不同，我们可以求 loss 的期望 (数学期望（或均值）是试验中每次可能结果的概率乘以其结果的总和，它反映随机变量平均取值的大小) 来获得 loss 的均值。

![W8 Bayes Optimality](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W8 Bayes Optimality.png)

注：这是条件数学期望，$E[(y-t)^2|x]$ 意为在给定 x 的下的 loss。t 为真实值，y 为预测值。

![W8 Bayes Optimality 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W8 Bayes Optimality 2.png)

现在我们可以得到 loss 的期望：

![W8 Bayes Optimality 3](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W8 Bayes Optimality 3.png)

bias (偏差)：期望和实际结果的差距。高偏差意味着模型的准确率很差，即欠拟合。
variance (方差)：模型和期望的方差。高方差意味着模型的泛化能力不好，即过拟合。

<img src="D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 Bias-variance.png" alt="W9 Bias-variance" style="zoom:67%;" />

## Bagging

从数据集中生成 m 个子数据集，求出它们的预测值的均值 $y=\frac{1}{m} \sum_{i=1}^{m} y_{i}$。

这样的操作给 loss 的期望带来以下影响：

![W9 Bagging](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 Bagging.png)

因此，bagging 的原理是通过这种操作降低 variance 来提升模型性能。

### The Idea

对于一个有 n 个数据的数据集，我们根据它生成 m 个同样大小的数据集 (里面的数据可以重复)。之后在这些数据集上进行训练，最后求所有模型的预测的平均值。这种方法叫做 bootstrap aggregation or bagging。

![W9 bagging 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 bagging 2.png)

![W9 bagging 3](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 bagging 3.png)

这样，我们就使用了均值 y 来充当最后的预测结果。

对于回归问题，bagging 使用均值来得到结果；对于分类问题，bagging   使用众数作为结果。



如果我们所有的数据集都是不相关的，我们可以极大的降低方差，但事实上并不是，因此降低的方差会变小。

![W9 bagging 4](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 bagging 4.png)

### Random Forests

随机森林：随机选取数据集中的一部分数据和特征，对于这些数据构建决策树。重复多次，多个决策树构成随机森林。

### Bagging Summary

![W9 bagging summary](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 bagging summary.png)

## Boosting

和 bagging 不同，boosting 致力于使集成的模型不相关 (decorrelates)，即学习弱点。为此，它使用了加权的训练集。

![W9 Weighted Training set](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 Weighted Training set.png)

这个意思是：对于上一个模型分错类的数据，我们要改变该数据的权重，使它在之后的训练中得到更多的重视。

Boosting 通过降低偏差来提升性能。

### AdaBoost (Adaptive Boosting)
AdaBoost 算法：

对于一个 base classifier，训练它并进行测试，将错误的数据进行 re-weight，用来训练新的 classifier。这样训练多个 weak classifiers / learners，最后将这些 weak classifier 以适当的权重集成到最终的模型里。

对于 base classifier，需要最小化它的 weighted error；同时，由于最终的模型可能很大，因此我们需要 base classifier 训练起来足够快 (不太考虑模型的性能)，因此我们使用决策树桩 (decision stump) 而不是决策树来充当 base classifier。

### Ada Boost Algorithm

![W9 Ada Boost Algorithm](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 Ada Boost Algorithm.png)

注：D~N~ 代表数据集有 N 个 feature (N 维)。最后的 0-1 loss 中，$h(x^{(n)})$ 和 $t^{(n)}$ 的结果都是 1 或 -1。因此 $\frac{1}{2}(1-h(x^{(n)})\cdot t^{(n)}))$ 的结果就为 0 和 1。

![W9 Ada Boost Algorithm 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 Ada Boost Algorithm 2.png)

注：最终的模型为 H(x)。一开始，我们给数据中 N 个 feature 分配的权重 w 都一样 ($\frac{1}{N}$)。最后一共训练了 t 个 weak classifiers。

$h_{t} \leftarrow \underset{h \in \mathcal{H}}{\operatorname{argmin}} \sum_{n=1}^{N} w^{(n)} \mathbb{I}\left\{h\left(\mathbf{x}^{(n)}\right) \neq t^{(n)}\right\}$ 代表用数据拟合出模型 $h_t$，argmin 代表使其 error 最小化。

$\operatorname{err}_{t}=\frac{\sum_{n=1}^{N} w^{(n)} \mathbb{I}\left\{h_{t}\left(\mathbf{x}^{(n)}\right) \neq t^{(n)}\right\}}{\sum_{n=1}^{N} w^{(n)}}$ 中，上面是错误数据的权重和，下面是所有权重的和 (即 1)。

$\alpha_t$ 是当前模型在最终模型中占的权重，即模型的系数。

在更新数据权重的时候 $w^{(n)} \leftarrow w^{(n)} \exp \left(2 \alpha_{t} \mathbb{I}\left\{h_{t}\left(\mathbf{x}^{(n)}\right) \neq t^{(n)}\right\}\right)$：

* 如果误差接近 0 ($err_t \approx  0$)，该模型的系数大，错误数据更受到重视
* 如果误差接近 0 ($err_t \approx  0.5$，这是二分类，0.5 就是啥都没干)，该模型的系数小，错误数据不受重视

最后得到的最终模型，是所有加权 weak learners 的相加。而误差最小的 learner 会得到最高的权重。

#### Example

![W9 AdaBoost example](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 AdaBoost example.png)

![W9 AdaBoost example 1](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 AdaBoost example 1.png)

![W9 AdaBoost example 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 AdaBoost example 2.png)

![W9 AdaBoost example 3](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 AdaBoost example 3.png)

![W9 AdaBoost example 4](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 AdaBoost example 4.png)

### Minimizes the Training Error

![W9 AdaBoost Minimizes the Training Error](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 AdaBoost Minimizes the Training Error.png)

注：如果 classifier 的 error 是 $\frac{1}{2}$，那么它就啥都没做。所以 error 应该小于 $\frac{1}{2}$，而 $\gamma$ 就是小了多少。

### Additive Models (公式的推导，没用)

现在用另一种方式解释 AdaBoost。

![W9 Additive Models 1](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 Additive Models 1.png)

注：这个实际上就是 boosting。

![W9 Additive Models 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 Additive Models 2.png)

注：m 之前的 H 都已经固定了，这里通过求最小 loss 找到 $h_m$ 和 $\alpha_m$。

![W9 Additive Models 3](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 Additive Models 3.png)

注：exponential loss 中 t 和 z 的值为 -1 或 1。因此，当预测准确时，结果是 -1，反之是 -1。$e^{-1}$ 接近 0，而 $e^{1}$ 很大。之后把 loss 带进去算，这里的 w 还是数据的权重。

![W9 Additive Models 4](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 Additive Models 4.png)

![W9 Additive Models 5](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 Additive Models 5.png)

![W9 Additive Models 6](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 Additive Models 6.png)

### Boosting summary

![W9 boosting summary](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 boosting summary.png)

一般的模型越复杂，越容易过拟合。但 boosting 中加入更多 weak classifiers，却不会过拟合。

## Summary

![W9 summary](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W9 summary.png)

