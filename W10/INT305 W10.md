# Probabilistic Models

## Maximum Likelihood Estimation

假如我们有一个硬币 (可能不公平)，掷了 N=100 次，得到了结果 $\{x_1, ... x_N\}$ ，其中 $x_{i} \in\{0,1\}$ (设 1 为正面向上)，并且正面向上的次数 $N_H=55$，$N_T=45$。

接下来我们希望建立模型来预测下一次掷硬币的结果。

由于硬币可能不公平，我们设结果 x 是 Bernoulli random variable，设得到 1 的概率为 $\theta$，$\theta \in\{0,1\}$。

<img src="D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 model 1.png" alt="W10 model 1" style="zoom:50%;" />

![W10 model 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 model 2.png)

注：上面是进行连乘。$\theta$ 是 0 和 1 之间的值。看到类似 (x|y) 的结构，就是知道 y，求 x，至于求 x 的什么，根据情况而定。

![W10 loss](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 loss.png)

注：似然和概率类似。不过概率描述了已知参数时的随机变量的输出结果；似然则用来描述已知随机变量输出结果时，未知参数的可能取值。上面的似然方程就是关于未知参数 $\theta$ 的。

之后我们要选择 $\theta$。根据 observed data，在我们取得似然函数的最大值时，对应的概率密度最大 (最合理)，因此我们需要 maximize likelihood：
$$
\hat{\theta}_{\mathrm{ML}}=\max _{\theta \in[0,1]} \ell(\theta)
$$

接下来得到 $\theta$ 的最大值：

![W10 Maximum Likelihood Estimation for the Coin Example](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 Maximum Likelihood Estimation for the Coin Example.png)

同时，我们还最小化了交叉熵：

![W10 Maximum Likelihood Estimation](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 Maximum Likelihood Estimation.png)

## Discriminative VS Generative

![W10 Discriminative VS Generative](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 Discriminative VS Generative.png)

两种方法：一种是直接建立决策边界的函数，不考虑分布；另一种是考虑分布进行分类。

## A Generative Model: Bayes Classifier

假如我们要对邮件分类：spam = 1，not spam = 0。

![W10 Bayes Classifier](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 Bayes Classifier.png)

注：bag-of-words 中的词如果出现在了邮件中，则值为 1，反之为 0。这些数据组成了向量 x。

![W10 Bayes Classifier 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 Bayes Classifier 2.png)

上面是根据邮件中出现的词来得到邮件属于哪个类的概率。$p(c|x)$ 表示根据词求类别的概率，$p(x|c)$ 代表词出现在不同类中的概率。

![W10 Bayes Classifier 3](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 Bayes Classifier 3.png)

### Naive Bayes

贝叶斯定理假设一个属性值对给定类的影响独立于其它属性的值，而此假设在实际情况中经常是不成立的，因此我们使用朴素贝叶斯，即假设给定目标值时属性之间相互条件独立。

![W10 Naive Bayes 0](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 Naive Bayes 0.png)

这样我们构建了一个 joint distribution (把类 c 加入了分布)，可以由此得到 $p(c)$ 和 $p(x|c)$。

### Bayes Nets

![W10  Bayes Nets](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10  Bayes Nets.png)

这种形式可以看作给定 (父) 变量的每个变量的条件分布的乘积。

#### Learning

![W10 learning](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 learning.png)

现在可以得到 $p(c)$ 和 $p(x|c)$。我们可以分开处理它们。

首先最大化 $\sum_{i=1}^{N} \log p\left(c^{(i)}\right)$：

假设 $p\left(c^{(i)}=1\right)=\pi$，那么 $p\left(c^{(i)}\right)=\pi^{c^{(i)}}(1-\pi)^{1-c^{(i)}}$。

![W10 learning 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 learning 2.png)

注：MLE 是“最大似然估计”。

接下来是最大化 $\sum_{i=1}^{N} \log p\left(x_{j}^{(i)} \mid c^{(i)}\right)$。

我们假设 $\theta_{j c}=p\left(x_{j}^{(i)}=1 \mid c\right)$，$\theta_{j c}$ 是第 j 个 x (词) 在类别 c 出现的似然。 $p\left(x_{j}^{(i)} \mid c\right)=\theta_{j c}^{x_{j}^{(i)}}\left(1-\theta_{j c}\right)^{1-x_{j}^{(i)}}$。

![W10 learning 3](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 learning 3.png)

#### Inference

![W10 Inference](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 Inference.png)

如果我们只想得到最可能的类别，就不需要计算分母。

![W10 Inference 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 Inference 2.png)

注：计算每个类的大小，取结构最大的那个类。

### MLE issue: Data Sparsity

MLE 有一个问题，就是如果数据太少，它会过拟合。这叫做数据稀疏性。

## Bayesian Parameter Estimation

前面介绍了贝叶斯分类器的一种参数估计方法“最大似然估计”，现在介绍另一种。它们的作用都是找到  $\theta$。

在最大似然中，我们把 observation 看作随机变量，但参数 $\theta$ 不是。但在贝叶斯方法中，我们把参数也看作随机变量。

为了建立 Bayesian model，我们需要指定两个分布：

* 先验分布 (prior distribution) $p(\theta)$，就是先随便给参数赋值。
* 似然 $p(\mathcal{D} \mid \boldsymbol{\theta})$，就是最大似然，$\mathcal{D}$ 是类别

之后，我们根据 observations 更新后验分布 (posterior distribution)：

<img src="D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 Bayesian Parameter Estimation 1.png" alt="W10 Bayesian Parameter Estimation 1" style="zoom: 50%;" />

我们一般不计算分母。

现在让我们回到最开始的问题-掷硬币，我们已经知道了它的似然：

<img src="D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 Bayesian Parameter Estimation 2.png" alt="W10 Bayesian Parameter Estimation 2" style="zoom:50%;" />

接下来需要指定它的先验分布 $p(\theta)$，这里我们设置了 beta distribution：
$$
p(\theta ; a, b)=\theta^{a-1}(1-\theta)^{b-1}
$$
注：a 和 b 是参数。

![W10 Bayesian Parameter Estimation 3](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 Bayesian Parameter Estimation 3.png)

接下来计算出后验分布：

![W10 Bayesian Parameter Estimation 4](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 Bayesian Parameter Estimation 4.png)

注：上面我们最后用期望得到了后验分布中的参数 $\theta$。

#### Maximum A-Posteriori Estimation

现在我们希望用另一种方式找到后验分布中最大的参数 $\theta$。

![W10 Maximum A Posteriori Estimation](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 Maximum A Posteriori Estimation.png)

![W10 Maximum A Posteriori Estimation 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 Maximum A Posteriori Estimation 2.png)

![W10 Maximum A Posteriori Estimation 3](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W10 Maximum A Posteriori Estimation 3.png)

