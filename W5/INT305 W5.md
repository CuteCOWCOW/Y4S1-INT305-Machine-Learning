# Neural Network and Back Propagation

## Activation functions

![W5 Activation functions](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W5 Activation functions.png)

## Gradient Descent

![W5 Gradient Descent](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W5 Gradient Descent.png)

注：对于第三个损失函数，$L_i$ 代表单个测试数据的损失；$\sum_{k} W_{k}^{2}$ 代表对权重的累加，这是一种正则化的方法，因为我们有很多种 W 可以选择，但太过复杂的 W 可能导致过拟合，因此使用这种方式可以使模型选择简单的 W。最后我们希望求出梯度。

![W5 Computational Graph](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W5 Computational Graph.png)

上面是 SVM 的计算图，R(W) 就是上面的 $\sum_{k} W_{k}^{2}$。

## BP

![W5 BP](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W5 BP.png)

上面是 BP 算法中 chain rule 的一个示例。

<img src="D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W5 sigmoid.png" alt="W5 sigmoid"  />

这里可以把 sigmoid 部分看作一个整体，求出梯度。

![W5 Pattern in backward flow](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W5 Pattern in backward flow.png)

加法门：梯度分配器，分配相同的梯度值。

最大值门：梯度路由，反向传播时，它会把梯度值分配给输入值最大的线路。

乘法门：梯度转换器，把梯度转换成不一样的值。

## Exercise

![W5 exercise](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W5 exercise.png)

第一问：

![W5 exercise 1](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W5 exercise 1.jpg)

第二问：

![W5 exercise 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W5 exercise 2.jpg)

## Gradients for vector

![W5 Gradients for vector](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W5 Gradients for vector.png)

之前输入单个值的时候，梯度是单个值；现在输入的是向量，梯度也变成同等大小的向量，即 jacobian matrix (其中是 local gradient)。

![W5 Gradients for vector 0](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W5 Gradients for vector 0.png)

注：jacobian matrix 的 shape 和输入的 shape 一致。

如下面的例子所示，我们对其进行 BP 算法：

![W5 Gradients for vector 2](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W5 Gradients for vector 2.png)

前向传播之后，进行反向传播：

![W5 Gradients for vector 3](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W5 Gradients for vector 3.png)

之后求 W 的梯度：

![W5 Gradients for vector 4](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W5 Gradients for vector 4.png)

注：比如要求 $W_{1,1}$ 的梯度，包含 $W_{1,1}$ 的式子只有 $q_1$，因为 $q_1 = W_{1,1}x_1 + ... W_{1,n}x_n$。

因此，可以得到：$\frac{\partial f}{\partial w_{i,j}} = \frac{\partial f}{\partial q_{k}} \cdot \frac{\partial q_k}{\partial w_{i,j}} = 2q_kx_j$。

最后是 x 的梯度：

![W5 Gradients for vector 5](D:\Files\Learning Materials\Y4\INT305-MachineLearning\Images for md\W5 Gradients for vector 5.png)

注：比如要求 $x_{1}$ 的梯度，所有 q 中都包含 $x_{1}$。因此我们要考虑所有的 q。

包含 $x_{1}$ 的式子：$f = (w_{1,1}x_1)^2 + ... +(w_{n,1}x_1)^2$，我们要求这个式子对 $x_{1}$ 的偏导，就要求出每一个的偏导，再相加。先求 $q_1$ 中 $x_{1}$ 的偏导：$\frac{\partial q_1}{\partial x_1} = 2(w_{1,1}x_1)w_{1,1} = 2q_1w_{1,1}$。

对每一个 q 都这样操作，f 对 $x_1$ 的偏导最后就是：$\frac{\partial f}{\partial x_{1}} = 2q_1w_{1,1} + ... + 2q_nw_{n,1} = \sum_{k} 2 q_{k} W_{k, 1}$。

因此，对于 i 个 x，最后就得到了 $\sum_{k} 2 q_{k} W_{k, i}$。

